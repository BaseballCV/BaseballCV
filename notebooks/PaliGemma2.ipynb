{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jshmH6orDyvw"
      },
      "source": [
        "#**PaliGemma2 Demo Notebook**\n",
        "\n",
        "\n",
        "---\n",
        "This notebook demonstrates how to use the PaliGemma2 class for computer vision tasks, specifically using the ball dataset.\n",
        "## Pre-work\n",
        "Let's make sure that we have access to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsjN79ksDyED"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_jWXsncE7ts"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd9_0DiSFCt4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTmzjEKcFLtM"
      },
      "source": [
        "##Create a work directory and cd into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBm2xWPgFG7D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "working_dir = \"paligemma2\"\n",
        "working_path = os.path.join('/content/drive/MyDrive', working_dir)\n",
        "os.makedirs(working_path, exist_ok=True)\n",
        "\n",
        "# Change directory to the working directory\n",
        "%cd {working_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXYlk92zFkz8"
      },
      "source": [
        "## Clone BaseballCV Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysnK9VVGFm4t"
      },
      "outputs": [],
      "source": [
        "!git clone -b 68-add-paligemma2-class-and-notebook https://github.com/dylandru/BaseballCV.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skYI5m5SFo4s"
      },
      "source": [
        "#Set as Current Directory and install requirements for BaseballCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo3GLq9NFzm7"
      },
      "outputs": [],
      "source": [
        "%cd BaseballCV\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbYHPy8mF8_p"
      },
      "source": [
        "##Due to needing to restart the session after the previous step, we need to redo this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu6q0wzqGIq3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "working_dir = \"paligemma2\"\n",
        "working_path = os.path.join('/content/drive/MyDrive', working_dir)\n",
        "os.makedirs(working_path, exist_ok=True)\n",
        "\n",
        "# Change directory to the working directory\n",
        "%cd {working_path}\n",
        "%cd BaseballCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8yCur7qGO9q"
      },
      "source": [
        "#Initialize PaliGemma2 Model\n",
        "\n",
        "\n",
        "---\n",
        "##Now let's initialize our PaliGemma2 model:\n",
        "\n",
        "*(You will need a HuggingFace token, and need to added to the Secrets section - the \"key\" section on the left -  by the name \"HF_TOKEN\". You also need to ask permision to use the paligemma model at: https://huggingface.co/google/paligemma2-3b-pt-224)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCpsdediGmBi"
      },
      "outputs": [],
      "source": [
        "from baseballcv.models import PaliGemma2\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN') or os.environ.get('HF_TOKEN')\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "if HF_TOKEN:\n",
        "  login(token=HF_TOKEN)\n",
        "else:\n",
        "  print(\"Warning: HF_TOKEN not found. You may need to request access to the model manually.\")\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "batch_size=4\n",
        "model = PaliGemma2(batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUMglF2vGr96"
      },
      "source": [
        "##Let's load the ball dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROY0fRHcG6pr"
      },
      "outputs": [],
      "source": [
        "from baseballcv.functions import LoadTools\n",
        "\n",
        "# Initialize LoadTools\n",
        "load_tools = LoadTools()\n",
        "\n",
        "# Load the ball dataset\n",
        "dataset_path = load_tools.load_dataset('baseball')\n",
        "\n",
        "# Define classes for the ball dataset\n",
        "classes = {\n",
        "    2: \"baseball\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a0Il2-bQIoV"
      },
      "source": [
        "# Fine-tuning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Let's fine-tune the model on the ball dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsD2nwMSQOiW"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "training_results = model.finetune(\n",
        "    dataset=dataset_path,\n",
        "    classes=classes,\n",
        "    train_test_split=(80, 10, 10),\n",
        "    epochs=1,  # 1 epochs for brevity\n",
        "    lr=1e-06,\n",
        "    save_dir=\"model_checkpoints\",\n",
        "    num_workers=4,\n",
        "    lora_r=8,\n",
        "    lora_scaling=12,\n",
        "    lora_dropout=0.05\n",
        ")\n",
        "\n",
        "print(\"Training Results:\")\n",
        "print(f\"Best Metric: {training_results['best_metric']}\")\n",
        "print(f\"Final Training Loss: {training_results['final_train_loss']}\")\n",
        "print(f\"Final Validation Loss: {training_results['final_val_loss']}\")\n",
        "print(f\"Model saved at: {training_results['model_path']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m7i6L3MQbt1"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Let's evaluate the model's performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORU6XRnDQfPd"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluation_results = model.evaluate(\n",
        "    base_path=dataset_path,\n",
        "    classes=classes,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"mAP: {evaluation_results.map50}\")\n",
        "print(f\"mAP@50:95: {evaluation_results.map}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2n1xK7jcUTY"
      },
      "source": [
        "# Visualizing Results with TensorBoard\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## You can visualize the training metrics using TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFWVMBFXceW0"
      },
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Launch TensorBoard\n",
        "%tensorboard --logdir {training_results['tensorboard_dir']}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
